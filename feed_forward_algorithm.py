# -*- coding: utf-8 -*-
"""feed_forward_algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QyirhNIVtGOSXi3xS2W2cS2HNWX-RjnJ
"""

import numpy as np
import pandas as pd

df = pd.read_csv('https://raw.githubusercontent.com/lukeyang01/nn-from-scratch/main/week2_data.csv')

inputs = np.array(df.iloc[:, 0:2])
labels = np.array(df.iloc[:, 2])

def sigmoid(x: int) -> float:
  """Sigmoid activation function."""
  return 1 / (1 + np.exp(-x))

def sigmoid_back(x: int) -> float:
  """Derivative of sigmoid for backward calculation."""
  fwd = sigmoid(x)
  return fwd * (1-fwd)

class MLP:
  """A Neural Network Class to Perform Basic Feedforward algorithm and training"""
  def __init__(self, sizes: list):
    """Initialize a numpy array or a list of weights an array or list of weights depending on sizes"""
    self.sizes = sizes
    self.num_layers = len(sizes)
    self.weights = []
    self.biases = []

    self.__init_params()
    return

  def __init_params(self):
    # iteration over network layers
      for i in range(1, self.num_layers):
          # X inputs -> Y Ouputs
          in_size = self.sizes[i-1]
          out_size = self.sizes[i]

          # weights.shape = (Y, X), biases.shape = (Y, 1)
          # e.g. 2 -> 3: weights is (2,3), biases is (1,3)
          self.weights.append(np.random.randn(in_size, out_size) * 0.1)
          self.biases.append(np.random.randn(1, out_size) * 0.1)
          # print(self.weights[-1].shape, self.biases[-1].shape)

  def forward(self, x: np.ndarray):
    """
      Perform feedforward algorithm on input vector for all layers

      Input:    x: np.ndarray with shape (1, self.sizes[0])

      Returns:  y: np.ndarray with shape (1, self.sizes[-1])
    """
    # Reshape the input vector to match specs this way inputs can be passed in any shape.
    x = x.reshape(1, self.sizes[0])


    # perform the feed forward algo
    # taking weighted sum
    # not taking final layer because output layer
    for i in range(self.num_layers - 1):
      x = np.matmul(x, self.weights[i]) + self.biases[i]  #add bias value to all values in weighted sum
      if(i < self.num_layers - 2):
        sigmoid(x)

    return x

  def backward(self, x, y):
    """Perform backpropagation using the input and expected output to get weight and vector deltas"""
    return False

  def train(self, X_train, y_train, epochs=1, lr=0.01, batch_size=1, verbose=True):
    """Using forward and backward functions, fit the model on an entire training step using gradient descent algorithm."""
    return False

def main():
  nn = MLP([2, 3, 2])
  for x in inputs:
    val = nn.forward(x)
    print(val)

main()

