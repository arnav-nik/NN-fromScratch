{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAQFzY_V8WG6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/lukeyang01/nn-from-scratch/main/week2_data.csv')\n",
        "\n",
        "inputs = np.array(df.iloc[:, 0:2])\n",
        "labels = np.array(df.iloc[:, 2])"
      ],
      "metadata": {
        "id": "jqtpz4LKlx1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x: int) -> float:\n",
        "  \"\"\"Sigmoid activation function.\"\"\"\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_back(x: int) -> float:\n",
        "  \"\"\"Derivative of sigmoid for backward calculation.\"\"\"\n",
        "  fwd = sigmoid(x)\n",
        "  return fwd * (1-fwd)"
      ],
      "metadata": {
        "id": "sYlAZMU4lKUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "  \"\"\"A Neural Network Class to Perform Basic Feedforward algorithm and training\"\"\"\n",
        "  def __init__(self, sizes: list):\n",
        "    \"\"\"Initialize a numpy array or a list of weights an array or list of weights depending on sizes\"\"\"\n",
        "    self.sizes = sizes\n",
        "    self.num_layers = len(sizes)\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "\n",
        "    self.__init_params()\n",
        "    return\n",
        "\n",
        "  def __init_params(self):\n",
        "    # iteration over network layers\n",
        "      for i in range(1, self.num_layers):\n",
        "          # X inputs -> Y Ouputs\n",
        "          in_size = self.sizes[i-1]\n",
        "          out_size = self.sizes[i]\n",
        "\n",
        "          # weights.shape = (Y, X), biases.shape = (Y, 1)\n",
        "          # e.g. 2 -> 3: weights is (2,3), biases is (1,3)\n",
        "          self.weights.append(np.random.randn(in_size, out_size) * 0.1)\n",
        "          self.biases.append(np.random.randn(1, out_size) * 0.1)\n",
        "          # print(self.weights[-1].shape, self.biases[-1].shape)\n",
        "\n",
        "  def forward(self, x: np.ndarray):\n",
        "    \"\"\"\n",
        "      Perform feedforward algorithm on input vector for all layers\n",
        "\n",
        "      Input:    x: np.ndarray with shape (1, self.sizes[0])\n",
        "\n",
        "      Returns:  y: np.ndarray with shape (1, self.sizes[-1])\n",
        "    \"\"\"\n",
        "    # Reshape the input vector to match specs this way inputs can be passed in any shape.\n",
        "    x = x.reshape(1, self.sizes[0])\n",
        "\n",
        "    # TODO: For each layer in network, perform feed forward algorithm\n",
        "    # Hint: Use np.matmul instead of np.dot here (1x2 input) * (2x3 weights) -> (1x3 output)\n",
        "    # Hint: The zip function may be helpful but is non nessecary\n",
        "\n",
        "    # taking weighted sum\n",
        "    # not taking final layer because output layer\n",
        "    for i in range(self.num_layers - 1):\n",
        "      x = np.matmul(x, self.weights[i]) + self.biases[i]  #add bias value to all values in weighted sum\n",
        "      if(i < self.num_layers - 2):\n",
        "        sigmoid(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def backward(self, x, y):\n",
        "    \"\"\"Perform backpropagation using the input and expected output to get weight and vector deltas\"\"\"\n",
        "    return False\n",
        "\n",
        "  def train(self, X_train, y_train, epochs=1, lr=0.01, batch_size=1, verbose=True):\n",
        "    \"\"\"Using forward and backward functions, fit the model on an entire training step using gradient descent algorithm.\"\"\"\n",
        "    return False"
      ],
      "metadata": {
        "id": "kTAMUv0r8sIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  nn = MLP([2, 3, 2])\n",
        "  for x in inputs:\n",
        "    val = nn.forward(x)\n",
        "    print(val)"
      ],
      "metadata": {
        "id": "v_FfESHQ9yEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfXnBZHHse_p",
        "outputId": "61d450b4-6f08-4b36-814e-83a71a98fbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.10889629  0.01988998]]\n",
            "[[-0.10768665  0.03094609]]\n",
            "[[-0.08582388  0.00186662]]\n",
            "[[-0.08580991 -0.00774502]]\n",
            "[[-0.11207798  0.03687545]]\n",
            "[[-0.08726455  0.00197428]]\n",
            "[[-0.09397937  0.01790574]]\n",
            "[[-0.09127501  0.01719781]]\n",
            "[[-0.11138122  0.05782356]]\n",
            "[[-0.10632265  0.01330984]]\n",
            "[[-0.1120985   0.02364944]]\n",
            "[[-0.10323067  0.03380017]]\n",
            "[[-0.10169593  0.02662525]]\n",
            "[[-0.10840692  0.0332394 ]]\n",
            "[[-0.08547397 -0.00481611]]\n",
            "[[-0.1200445   0.05425127]]\n",
            "[[-0.113835    0.02635484]]\n",
            "[[-0.11509281  0.03229072]]\n",
            "[[-0.11103557  0.02262086]]\n",
            "[[-0.11389651  0.04071255]]\n",
            "[[-0.10070053  0.02619377]]\n",
            "[[-0.1043789   0.03097297]]\n",
            "[[-0.0954877   0.00217783]]\n",
            "[[-0.09133477 -0.00353342]]\n",
            "[[-0.11404006  0.05178668]]\n",
            "[[-0.10933249  0.02435373]]\n",
            "[[-0.09888654  0.02949547]]\n",
            "[[-0.1128876   0.04990975]]\n",
            "[[-0.10077374  0.03937019]]\n",
            "[[-0.10555694  0.02492213]]\n",
            "[[-0.10478113  0.03662906]]\n",
            "[[-0.09383393  0.00331081]]\n",
            "[[-0.11503578  0.04362004]]\n",
            "[[-0.0976848   0.02254901]]\n",
            "[[-0.10597159  0.03156701]]\n",
            "[[-0.08690003  0.00884517]]\n",
            "[[-0.11902372  0.05599386]]\n",
            "[[-0.12252042  0.04148568]]\n",
            "[[-0.10160087  0.00512172]]\n",
            "[[-0.10996665  0.02379878]]\n",
            "[[-0.12110309  0.05387227]]\n",
            "[[-0.08975979  0.02067284]]\n",
            "[[-0.10580198  0.02237548]]\n",
            "[[-0.10912726  0.02339444]]\n",
            "[[-0.0866559  -0.00531498]]\n",
            "[[-0.1207007   0.05804615]]\n",
            "[[-0.12428792  0.05113397]]\n",
            "[[-0.12001303  0.0235742 ]]\n",
            "[[-0.10273105  0.00449051]]\n",
            "[[-0.09776315 -0.07560694]]\n",
            "[[-0.08479037 -0.08433049]]\n",
            "[[-0.08489374 -0.06253291]]\n",
            "[[-0.10607605 -0.05757212]]\n",
            "[[-0.07653081 -0.07247862]]\n",
            "[[-0.07529609 -0.07924548]]\n",
            "[[-0.07483453 -0.08631005]]\n",
            "[[-0.089293  -0.0748033]]\n",
            "[[-0.08352405 -0.06924969]]\n",
            "[[-0.10395679 -0.05869577]]\n",
            "[[-0.07791907 -0.07016921]]\n",
            "[[-0.09068915 -0.06767784]]\n",
            "[[-0.10702432 -0.05488714]]\n",
            "[[-0.07641793 -0.08731546]]\n",
            "[[-0.07930228 -0.09416621]]\n",
            "[[-0.07851019 -0.08090876]]\n",
            "[[-0.07659259 -0.08861971]]\n",
            "[[-0.07415025 -0.11016832]]\n",
            "[[-0.0741988  -0.08329929]]\n",
            "[[-0.09237594 -0.07011561]]\n",
            "[[-0.07349247 -0.07793483]]\n",
            "[[-0.10289507 -0.06275151]]\n",
            "[[-0.09117551 -0.0750341 ]]\n",
            "[[-0.0672819  -0.08339913]]\n",
            "[[-0.08306898 -0.07715843]]\n",
            "[[-0.07884987 -0.09969743]]\n",
            "[[-0.07989776 -0.08673612]]\n",
            "[[-0.08856844 -0.07238589]]\n",
            "[[-0.09004489 -0.07363355]]\n",
            "[[-0.09426819 -0.07337732]]\n",
            "[[-0.07047388 -0.10198927]]\n",
            "[[-0.06455284 -0.09147492]]\n",
            "[[-0.07491164 -0.08056544]]\n",
            "[[-0.06882668 -0.08157199]]\n",
            "[[-0.09898974 -0.0742716 ]]\n",
            "[[-0.08373092 -0.08255305]]\n",
            "[[-0.09823864 -0.0576512 ]]\n",
            "[[-0.08582805 -0.06811747]]\n",
            "[[-0.09727324 -0.06050062]]\n",
            "[[-0.06231191 -0.09141022]]\n",
            "[[-0.07637845 -0.09656348]]\n",
            "[[-0.08294537 -0.10697973]]\n",
            "[[-0.08135906 -0.10524565]]\n",
            "[[-0.06584856 -0.07056413]]\n",
            "[[-0.08644386 -0.09254157]]\n",
            "[[-0.0723852 -0.1030601]]\n",
            "[[-0.08767316 -0.07576999]]\n",
            "[[-0.09540787 -0.05134646]]\n",
            "[[-0.08693606 -0.06962381]]\n",
            "[[-0.07496009 -0.08813096]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UN-GskfktKDt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}